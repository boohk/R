
# How we do crawling many pages?
# We can do crawling using for statement!


#다음 리뷰 크롤링
final_data=NULL

for(i in 1:300)
{
  url=paste0(http://movie.daum.net/moviedb/grade?movieId=95306&type=netizen&page=",i)
  line=readLines(url,encoding="UTF-8")

  inx=str_detect(line,"<p class=\"desc_review\">")  #\붙여서 R이 "를 문자로 인식하게 해줌
  title=line[which(inx)+1]
  title=gsub('\t|<.*?>','',title)
  title=str_trim(title)
  
  cat(i,'page complete\n')
  final_data=cbind(final_data,title)
}

final_data = as.character(final_data)
final_data = data.frame(final_data,stringsAsFactors = F)
write.csv(final_data, 'naver_review.csv')
write.table(final_data,'naver_review.txt')
